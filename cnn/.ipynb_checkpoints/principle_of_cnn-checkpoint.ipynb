{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络的原理和结构\n",
    "**1.局部性**\n",
    "图片的局部区域可以决定图片的特征，如一张鸟的图片，鸟嘴的部分可以确定这是个鸟类的图片\n",
    "\n",
    "**2.相同性**\n",
    "可以用同样的检测模式去检测不同图片的相同特征\n",
    "\n",
    "**3.不变性**\n",
    "对一个大图片进行采样，图片的性质基本保持不变\n",
    "\n",
    "全连接神经网络对于图片而言会导致参数增加特别快，卷积神经网络是一个3D容量的神经元，是以三个维度来排列的：宽度，高度和深度。\n",
    "\n",
    "卷积神经网络的主要层结构有三个，卷积层，池化层和全连接层，通过堆叠形成一个完整的卷积神经网络结构。卷积神经网络将图片转化为最后的类别得分，通过梯度下降法更新参数（卷积层和全连接层拥有参数，激活层和池化层不含参数），使得神经网络能识别出图片类别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.卷积层\n",
    "卷积层的参数是由一些可学习的滤波器集合构成的，每个滤波器在空间上都比较小，每个滤波器沿着宽和高移动，假设参数有二十个滤波器，生成20张激活图，叠加起来就是卷积层的输出。<br/>\n",
    "每个神经元只与输入数据的一个局部连接（因为全连接的话太大了，mnist例子中的就是784，这还是小图片），可以局部连接的原因是因为**局部性**,神经元的感受野与神经元相连接(感受野在数字图像处理中讲是神经元变化引起刺激的那一个区域)，在卷积神经网络中，感受野的大小就是滤波器的宽和高，深度和输入深度相等。<br/>\n",
    "卷积层的输出深度代表有多少个滤波器对数据进行处理，每种滤波器寻找一种特征进行激活。步长代表滤波器一次移动几个像素点，边界填充代表用数据0在原图边界填充，控制输出数据的尺寸，输出的尺寸为$ z = \\frac{W-F+2P}{S}+1 $ W代表输入数据大小，F代表滤波器大小，P代表填充，S代表步长（一般设置为F=3，S=1，P=1）。根据上式可知，如果想要输入输出尺寸相同，当S=1时，$ P = \\frac{F-1}{2} $,对超参数的设置要保证上式最后输出为整数。<br/>\n",
    "由于一个滤波器如果可以检测出一个位置的特征，也可以检测出另一个位置的特征，所以，卷积层一共有32个滤波器每个滤波器参数为3×3×10，总共参数就是32×3×3×10，就不需要考虑卷积层的尺寸大小了。\n",
    "<br/>\n",
    "在输出体中，第d个深度切片，用第d个滤波器和输入数据进行有效卷积运算，再加上第d个偏置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.池化层\n",
    "池化层每次都从2×2的窗口中选择最大的数值，达到缩小图片尺寸的目的。最大池化一般采取的参数是空间大小为3,步长为2,这种有重叠，或者空间大小为2,步长为2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.全连接层\n",
    "和一般神经网络结构相同，在经历了卷积层和池化层之后，全连接层每个神经元与前一层所有的相连接。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积神经网络的基本形式是将一些卷积层+批量标准化+ReLU激活，然后紧跟池化层，逐渐缩小尺寸，然后是几个全连接层特征展开。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch卷积模块\n",
    "### 1.卷积层\n",
    "nn.Conv2d() 常用参数：in_channels 输入数据体的深度,out_channels 输出数据体的深度,kernel_size 卷积核大小,stride 步长,padding 填充。\n",
    "### 2.池化层\n",
    "nn.MaxPool2d()表示网络中的最大值池化，kernerl_size,stride,padding,dilation表示卷积对于输入数据体的空间间隔。return_indices表示是否返回最大值所处下标。<br/>\n",
    "nn.AvgPool2d()表示均值池化，count_include_pad表示是否包含零填充。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构造一个简单多层卷积神经网络\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN,self).__init__()#假设输入尺寸为3×32×32\n",
    "        layer1=nn.Sequential()\n",
    "        layer1.add_module('conv1',nn.Conv2d(3,32,3,1,padding=1))#输入深度为3,输出深度为32，说明有32个滤波器，大小为3×3,步长1\n",
    "        #32×32×32\n",
    "        layer1.add_module('relu1',nn.ReLU(True))\n",
    "        layer1.add_module('pool1',nn.MaxPool2d(2,2))#最大池化，尺寸为2×2,步长为1\n",
    "        #32×16×16\n",
    "        self.layer1=layer1\n",
    "    \n",
    "        layer2=nn.Sequential()\n",
    "        layer2.add_module('conv2',nn.Conv2d(32,64,3,1,padding=1))\n",
    "        #64×16×16\n",
    "        layer2.add_module('relu2',nn.ReLU(True))\n",
    "        layer2.add_module('pool2',nn.MaxPool2d(2,2))\n",
    "        #64×8×8\n",
    "        self.layer2=layer2\n",
    "        \n",
    "        layer3=nn.Sequential()\n",
    "        layer3.add_module('conv3',nn.Conv2d(64,128,3,1,padding=1))\n",
    "        #128×8×8\n",
    "        layer3.add_module('relu3',nn.ReLU(True))\n",
    "        layer3.add_module('pool3',nn.MaxPool2d(2,2))\n",
    "        #128×4×4=2048\n",
    "        self.layer3=layer3\n",
    "        #全连接层\n",
    "        layer4=nn.Sequential()\n",
    "        layer4.add_module('fc1',nn.Linear(2048,512))\n",
    "        layer4.add_module('fc_relu1',nn.ReLU(True))\n",
    "        layer4.add_module('fc2',nn.Linear(512,64))\n",
    "        layer4.add_module('fc_relu2',nn.ReLU(True))\n",
    "        layer4.add_module('fc3',nn.Linear(64,10))\n",
    "        self.layer4=layer4\n",
    "        \n",
    "    def forward(self,x):\n",
    "        conv1=self.layer1(x)\n",
    "        conv2=self.layer2(conv1)\n",
    "        conv3=self.layer3(conv2)\n",
    "        fc_input=conv3.view(conv3.size(0),-1)\n",
    "        fc_out=self.layer4(fc_input)\n",
    "        return fc_out\n",
    "model=SimpleCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (layer1): Sequential(\n",
      "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu1): ReLU(inplace)\n",
      "    (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu2): ReLU(inplace)\n",
      "    (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu3): ReLU(inplace)\n",
      "    (pool3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (fc1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (fc_relu1): ReLU(inplace)\n",
      "    (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
      "    (fc_relu2): ReLU(inplace)\n",
      "    (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU(inplace)\n",
       "    (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU(inplace)\n",
       "    (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#提取层结构，children()和modules()函数,modules可以访问到层内部\n",
    "new_model=nn.Sequential(*list(model.children())[:2])\n",
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (layer1.conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (layer2.conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (layer3.conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model=nn.Sequential()\n",
    "for layer in model.named_modules():\n",
    "    if isinstance(layer[1],nn.Conv2d):\n",
    "        conv_model.add_module(layer[0],layer[1])\n",
    "conv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.conv1.weight\n",
      "layer1.conv1.bias\n",
      "layer2.conv2.weight\n",
      "layer2.conv2.bias\n",
      "layer3.conv3.weight\n",
      "layer3.conv3.bias\n",
      "layer4.fc1.weight\n",
      "layer4.fc1.bias\n",
      "layer4.fc2.weight\n",
      "layer4.fc2.bias\n",
      "layer4.fc3.weight\n",
      "layer4.fc3.bias\n"
     ]
    }
   ],
   "source": [
    "#提取参数及自定义初始化\n",
    "for param in model.named_parameters():\n",
    "    print(param[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import init\n",
    "for m in model.modules():\n",
    "    if isinstance(m,nn.Conv2d):\n",
    "        init.normal(m.weight.data)\n",
    "        init.xavier_normal(m.weight.data)\n",
    "        init.kaiming_normal(m.weight.data)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif isinstance(m,nn.Linear):\n",
    "        m.weight.data.normal_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "使用方法跟一般神经网络差不多，卷积神经网络大概就是对一个很大的数据集进行了缩减吧。一系列预处理，然后再神经网络。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
